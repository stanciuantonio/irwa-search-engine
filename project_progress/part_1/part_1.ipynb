{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2461e3d8",
   "metadata": {},
   "source": [
    "1. As a first step, you must pre-process the documents. In particular, for the text fields (title,\n",
    "description) you should:\n",
    "● Removing stop words\n",
    "● Tokenization\n",
    "● Removing punctuation marks\n",
    "● Stemming\n",
    "● and... anything else you think it's needed (bonus point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfb2a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b8cb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66ba8c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total products in dataset: 28080\n",
      "Sample product title: Solid Women Multicolor Track Pants\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '../../data/fashion_products_dataset.json'\n",
    "with open(dataset_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(f\"Total products in dataset: {len(data)}\")\n",
    "\n",
    "data_sample_title = data[0]['title']\n",
    "print(f\"Sample product title: {data_sample_title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e284b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted product details: 1005COMBO2 Elastic Side Pockets Cotton Blend Solid Multicolor\n"
     ]
    }
   ],
   "source": [
    "def extract_product_details(details):\n",
    "    \"\"\"\n",
    "    Extracts only the descriptive values from structured product_details.\n",
    "    Example input: [{\"Color\": \"Blue\"}, {\"Material\": \"Cotton\"}]\n",
    "    Output: \"Blue Cotton\"\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for category in details:\n",
    "        values.extend(v for v in category.values())\n",
    "    return \" \".join(values)\n",
    "\n",
    "sample_details = data[0]['product_details']\n",
    "extracted_details = extract_product_details(sample_details)\n",
    "print(f\"Extracted product details: {extracted_details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9dda43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document terms: ['solid', 'women', 'multicolor', 'track', 'pant', 'yorker', 'trackpant', 'made', '100', 'rich', 'comb', 'cotton', 'give', 'rich', 'look', 'design', 'comfort', 'skin', 'friendli', 'fabric', 'itch', 'free', 'waistband', 'great', 'year', 'round', 'use', 'proudli', 'made', 'india', '1005combo2', 'elast', 'side', 'pocket', 'cotton', 'blend', 'solid', 'multicolor']\n"
     ]
    }
   ],
   "source": [
    "def build_terms(document):\n",
    "    \"\"\"\n",
    "    Preprocess the document text (title + description + product_details extracted) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "\n",
    "    Argument:\n",
    "    document -- a dictionary with 'title' and 'description' keys\n",
    "\n",
    "    Returns:\n",
    "    tokens - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    text = document['title'] + ' ' + document['description'] + ' ' + extract_product_details(document['product_details'])\n",
    "    text = text.lower()\n",
    "    text = ''.join(char if char.isalnum() or char.isspace() else ' ' for char in text)\n",
    "    text = text.split(\" \")\n",
    "    text = [term for term in text if term not in stop_words]\n",
    "    text = [term for term in text if term != '']\n",
    "    text = [stemmer.stem(term) for term in text]\n",
    "    return text\n",
    "\n",
    "sample_document = data[0]\n",
    "sample_terms = build_terms(sample_document)\n",
    "print(f\"Sample document terms: {sample_terms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e04c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document metadata: ['clothing', 'accessories', 'bottomwear', 'york', 'shyam', 'enterprises']\n"
     ]
    }
   ],
   "source": [
    "def build_metadata(document):\n",
    "    \"\"\"\n",
    "    Preprocess the document other fields (category, sub_category, brand, seller) removing stop words,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "\n",
    "    Argument:\n",
    "    document -- a dictionary with 'category', 'sub_category', 'brand', 'seller' keys\n",
    "\n",
    "    Returns:\n",
    "    tokens - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    text = document['category'] + ' ' + document['sub_category'] + ' ' + document['brand'] + ' ' + document['seller']\n",
    "    text = text.lower()\n",
    "    text = ''.join(char if char.isalnum() or char.isspace() else ' ' for char in text)\n",
    "    text = text.split(\" \")\n",
    "    text = [term for term in text if term not in stop_words]\n",
    "    text = [term for term in text if term != '']\n",
    "    return text\n",
    "\n",
    "sample_document = data[0]\n",
    "sample_metadata = build_metadata(sample_document)\n",
    "print(f\"Sample document metadata: {sample_metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd6f4005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total preprocessed products: 28080\n"
     ]
    }
   ],
   "source": [
    "def preprocess_document(document):\n",
    "    tokens = build_terms(document)\n",
    "    metadata_tokens = build_metadata(document)\n",
    "    return {\n",
    "        'pid': document['pid'],\n",
    "        'tokens': tokens,\n",
    "        'metadata_tokens': metadata_tokens,\n",
    "        'title': document['title'],\n",
    "        'description': document['description'],\n",
    "        'brand': document['brand'],\n",
    "        'category': document['category'],\n",
    "        'sub_category': document['sub_category'],\n",
    "        'product_details': document['product_details'],\n",
    "        'seller': document['seller'],\n",
    "        'out_of_stock': document['out_of_stock'],\n",
    "        'selling_price': document['selling_price'],\n",
    "        'discount': document['discount'],\n",
    "        'actual_price': document['actual_price'],\n",
    "        'average_rating': document['average_rating'],\n",
    "        'url': document['url']\n",
    "    }\n",
    "\n",
    "preprocessed_data = [preprocess_document(doc) for doc in data]\n",
    "print(f\"Total preprocessed products: {len(preprocessed_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irwa_venv (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
